# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Python Code to Count Tokens in Alpaca Dataset using Qwen2.5-0.5B Tokenizer

# Generated by Claude 3.7 Sonnet via Bedrock Playground:
#
# I need Python code to load the Alpaca dataset from the Hugging Face hub, run over
# all training sequences and return a list of number of tokens per sequence. The
# tokenizer should be the one coming with the Qwen2.5-0.5B model from Hugging Face
#
# This code:
#
#     Loads the Qwen2.5-0.5B tokenizer from Hugging Face
#     Loads the Alpaca dataset from "tatsu-lab/alpaca"
#     Processes each training example by:
#         Combining instruction, input, and output fields into a single text
#         Tokenizing using the Qwen2.5-0.5B tokenizer
#         Counting the number of tokens
#     Returns a list with the token count for each training sequence
#     Prints basic statistics about token counts and their distribution
#
# The script will output statistics about the token length distribution across the
# dataset, which can be helpful for understanding sequence length requirements for
# fine-tuning.
from typing import Dict
from datasets import load_dataset
from transformers import AutoTokenizer
import numpy as np
from tqdm import tqdm


DATASETS = {
    "alpaca": "tatsu-lab/alpaca",
    "longbench-v2": "THUDM/LongBench-v2",
}


def format_text(example: Dict[str, str], dataset: str) -> str:
    if dataset == "alpaca":
        text = f"Instruction: {example['instruction']}"
        if example["input"]:
            text += f"\nInput: {example['input']}"
        text += f"\nOutput: {example['output']}"
    else:
        # From https://huggingface.co/datasets/THUDM/LongBench-v2:
        # {
        #    "_id": "Unique identifier for each piece of data",
        #    "domain": "The primary domain category of the data",
        #    "sub_domain": "The specific sub-domain category within the domain",
        #    "difficulty": "The difficulty level of the task, either 'easy' or 'hard'",
        #    "length": "The length category of the task, which can be 'short', 'medium', or 'long'",
        #    "question": "The input/command for the task, usually short, such as questions in QA, queries in many-shot learning, etc",
        #    "choice_A": "Option A", "choice_B": "Option B", "choice_C": "Option C", "choice_D": "Option D",
        #    "answer": "The groundtruth answer, denoted as A, B, C, or D",
        #    "context": "The long context required for the task, such as documents, books, code repositories, etc."
        # }
        #
        # Prompt is from:
        # https://github.com/THUDM/LongBench/blob/main/prompts/0shot.txt
        text = "\n".join(
            [
                "Please read the following text and answer the question below.",
                "",
                "<text>",
                f"{example['context']}",
                "</text>",
                "",
                f"What is the correct answer to this question: {example['question']}",
                "Choices:",
                f"(A) {example['choice_A']}",
                f"(B) {example['choice_B']}",
                f"(C) {example['choice_C']}",
                f"(D) {example['choice_D']}",
                "",
                'Format your response as follows: "The correct answer is (insert answer here)".',
                "",
                "Answer:",
                f"The correct answer is {example['answer']}",
            ]
        )
    return text


def count_tokens_in_alpaca(dataset: str):
    # Load the Qwen2.5-0.5B tokenizer from Hugging Face
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B")

    if dataset == "alpaca":
        # Load the Alpaca dataset
        alpaca_dataset = load_dataset(DATASETS[dataset])
        # Get the training split
        train_data = alpaca_dataset["train"]
    else:
        train_data = load_dataset(DATASETS[dataset], split="train")

    # Initialize a list to store token counts
    token_counts = []

    # Process each training example
    for example in tqdm(train_data):
        text = format_text(example, dataset)
        # Tokenize the text and count tokens
        tokens = tokenizer(text, truncation=False)
        token_count = len(tokens.input_ids)
        token_counts.append(token_count)

    return token_counts


if __name__ == "__main__":
    # dataset = "alpaca"
    dataset = "longbench-v2"
    store_counts = True
    token_counts = count_tokens_in_alpaca(dataset)

    # Print some statistics
    print(f"Total sequences: {len(token_counts)}")
    print(f"Average tokens per sequence: {np.mean(token_counts):.2f}")
    print(f"Median tokens per sequence: {np.median(token_counts):.2f}")
    print(f"Min tokens per sequence: {np.min(token_counts)}")
    print(f"Max tokens per sequence: {np.max(token_counts)}")

    # Display histogram information
    print("\nToken count distribution:")
    max_len = max(token_counts) + 1
    step = max_len // 10
    bins = list(range(0, max_len, step)) + [max_len]
    hist, _ = np.histogram(token_counts, bins=bins)

    for i in range(len(hist)):
        print(f"{bins[i]}-{bins[i + 1] - 1}: {hist[i]} sequences")

    if store_counts:
        with open(f"{dataset}_token_counts.txt", "w") as fid:
            fid.writelines(f"{x}\n" for x in sorted(token_counts))
